{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_music_genaration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anipnwr7777/music_generation_using_deep_learning/blob/main/lstm_music_genaration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVjXIAVgBerC"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2HDQSYBBqkz"
      },
      "source": [
        "data_directory = \"/content/my_proj\"\n",
        "data_file = \"Data_Tunes.txt\"\n",
        "charIndex_json = \"char_to_index.json\"\n",
        "model_weights_directory = \"/content/my_proj/Model_Weights\"\n",
        "BATCH_SIZE = 16\n",
        "SEQ_LENGTH = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr86mgHwC8qp"
      },
      "source": [
        "def read_batches(all_chars, unique_chars):\n",
        "    length = all_chars.shape[0]\n",
        "    batch_chars = int(length / BATCH_SIZE) \n",
        "    print(length)\n",
        "\n",
        "    \n",
        "    for start in range(0, batch_chars - SEQ_LENGTH, 64):\n",
        "        X = np.zeros((BATCH_SIZE, SEQ_LENGTH))    \n",
        "        Y = np.zeros((BATCH_SIZE, SEQ_LENGTH, unique_chars))  \n",
        "        for batch_index in range(0, 16):  \n",
        "            for i in range(0, 64): \n",
        "                X[batch_index, i] = all_chars[batch_index * batch_chars + start + i]\n",
        "                Y[batch_index, i, all_chars[batch_index * batch_chars + start + i + 1]] = 1\n",
        "        yield X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm3MQc8uDC99"
      },
      "source": [
        "def built_model(batch_size, seq_length, unique_chars):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(input_dim = unique_chars, output_dim = 512, batch_input_shape = (batch_size, seq_length))) \n",
        "    \n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    \n",
        "    model.add(TimeDistributed(Dense(unique_chars)))\n",
        "\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4Ic2Q_YDMRa"
      },
      "source": [
        "\n",
        "def training_model(data, epochs = 5):\n",
        "    \n",
        "    char_to_index = {ch: i for (i, ch) in enumerate(sorted(list(set(data))))}\n",
        "    print(\"Number of unique characters in our whole tunes database = {}\".format(len(char_to_index))) \n",
        "    \n",
        "    with open(os.path.join(data_directory, charIndex_json), mode = \"w\") as f:\n",
        "        json.dump(char_to_index, f)\n",
        "        \n",
        "    index_to_char = {i: ch for (ch, i) in char_to_index.items()}\n",
        "    unique_chars = len(char_to_index)\n",
        "    \n",
        "    model = built_model(BATCH_SIZE, SEQ_LENGTH, unique_chars)\n",
        "    model.summary()\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
        "    \n",
        "    all_characters = np.asarray([char_to_index[c] for c in data], dtype = np.int32)\n",
        "    print(\"Total number of characters = \"+str(all_characters.shape[0])) \n",
        "    \n",
        "    epoch_number, loss, accuracy = [], [], []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
        "        final_epoch_loss, final_epoch_accuracy = 0, 0\n",
        "        epoch_number.append(epoch+1)\n",
        "        \n",
        "        for i, (x, y) in enumerate(read_batches(all_characters, unique_chars)):\n",
        "            final_epoch_loss, final_epoch_accuracy = model.train_on_batch(x, y) \n",
        "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, final_epoch_loss, final_epoch_accuracy))\n",
        "           \n",
        "        loss.append(final_epoch_loss)\n",
        "        accuracy.append(final_epoch_accuracy)\n",
        "        \n",
        "       \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            if not os.path.exists(model_weights_directory):\n",
        "                os.makedirs(model_weights_directory)\n",
        "            model.save_weights(os.path.join(model_weights_directory, \"Weights_{}.h5\".format(epoch+1)))\n",
        "            print('Saved Weights at epoch {} to file Weights_{}.h5'.format(epoch+1, epoch+1))\n",
        "    \n",
        "   \n",
        "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
        "    log_frame[\"Epoch\"] = epoch_number\n",
        "    log_frame[\"Loss\"] = loss\n",
        "    log_frame[\"Accuracy\"] = accuracy\n",
        "    log_frame.to_csv(\"/content/my_proj/log.csv\", index = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FS7B-_8NDPqA",
        "outputId": "ce2c09ce-9d7e-45b2-c4e9-730a960f6b7c"
      },
      "source": [
        "file = open(os.path.join(data_directory, data_file), mode = 'r')\n",
        "data = file.read()\n",
        "file.close()\n",
        "if __name__ == \"__main__\":\n",
        "    training_model(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of unique characters in our whole tunes database = 88\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (16, 64, 512)             45056     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (16, 64, 256)             787456    \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (16, 64, 256)             525312    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (16, 64, 256)             525312    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (16, 64, 256)             0         \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (16, 64, 88)              22616     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (16, 64, 88)              0         \n",
            "=================================================================\n",
            "Total params: 1,905,752\n",
            "Trainable params: 1,905,752\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Total number of characters = 192782\n",
            "Epoch 1/5\n",
            "192782\n",
            "Batch: 1, Loss: 4.47720193862915, Accuracy: 0.0166015625\n",
            "Batch: 2, Loss: 4.450628280639648, Accuracy: 0.1787109375\n",
            "Batch: 3, Loss: 4.383852958679199, Accuracy: 0.1396484375\n",
            "Batch: 4, Loss: 4.2089104652404785, Accuracy: 0.11328125\n",
            "Batch: 5, Loss: 3.84305477142334, Accuracy: 0.1591796875\n",
            "Batch: 6, Loss: 3.677018404006958, Accuracy: 0.15625\n",
            "Batch: 7, Loss: 3.591061592102051, Accuracy: 0.0888671875\n",
            "Batch: 8, Loss: 3.7229106426239014, Accuracy: 0.0673828125\n",
            "Batch: 9, Loss: 3.6207029819488525, Accuracy: 0.0771484375\n",
            "Batch: 10, Loss: 3.462085008621216, Accuracy: 0.103515625\n",
            "Batch: 11, Loss: 3.476670503616333, Accuracy: 0.119140625\n",
            "Batch: 12, Loss: 3.389509916305542, Accuracy: 0.126953125\n",
            "Batch: 13, Loss: 3.293405532836914, Accuracy: 0.146484375\n",
            "Batch: 14, Loss: 3.644019365310669, Accuracy: 0.111328125\n",
            "Batch: 15, Loss: 3.6616759300231934, Accuracy: 0.10546875\n",
            "Batch: 16, Loss: 3.2995400428771973, Accuracy: 0.1337890625\n",
            "Batch: 17, Loss: 3.300035238265991, Accuracy: 0.1455078125\n",
            "Batch: 18, Loss: 3.6542043685913086, Accuracy: 0.12109375\n",
            "Batch: 19, Loss: 3.632795810699463, Accuracy: 0.099609375\n",
            "Batch: 20, Loss: 3.5151004791259766, Accuracy: 0.09765625\n",
            "Batch: 21, Loss: 3.400862455368042, Accuracy: 0.0908203125\n",
            "Batch: 22, Loss: 3.337648391723633, Accuracy: 0.13671875\n",
            "Batch: 23, Loss: 3.426238536834717, Accuracy: 0.150390625\n",
            "Batch: 24, Loss: 3.483264446258545, Accuracy: 0.1435546875\n",
            "Batch: 25, Loss: 3.490201711654663, Accuracy: 0.142578125\n",
            "Batch: 26, Loss: 3.2586591243743896, Accuracy: 0.1669921875\n",
            "Batch: 27, Loss: 3.2519140243530273, Accuracy: 0.1845703125\n",
            "Batch: 28, Loss: 3.217500925064087, Accuracy: 0.16796875\n",
            "Batch: 29, Loss: 3.3817107677459717, Accuracy: 0.1513671875\n",
            "Batch: 30, Loss: 3.4241180419921875, Accuracy: 0.1220703125\n",
            "Batch: 31, Loss: 3.3310794830322266, Accuracy: 0.140625\n",
            "Batch: 32, Loss: 3.2573752403259277, Accuracy: 0.1611328125\n",
            "Batch: 33, Loss: 3.313410520553589, Accuracy: 0.158203125\n",
            "Batch: 34, Loss: 3.3034567832946777, Accuracy: 0.15234375\n",
            "Batch: 35, Loss: 3.2597150802612305, Accuracy: 0.162109375\n",
            "Batch: 36, Loss: 3.254646062850952, Accuracy: 0.158203125\n",
            "Batch: 37, Loss: 3.385780096054077, Accuracy: 0.1279296875\n",
            "Batch: 38, Loss: 3.2831859588623047, Accuracy: 0.150390625\n",
            "Batch: 39, Loss: 3.380803108215332, Accuracy: 0.1572265625\n",
            "Batch: 40, Loss: 3.3299739360809326, Accuracy: 0.1630859375\n",
            "Batch: 41, Loss: 3.23148775100708, Accuracy: 0.1787109375\n",
            "Batch: 42, Loss: 3.3283944129943848, Accuracy: 0.1552734375\n",
            "Batch: 43, Loss: 3.305697202682495, Accuracy: 0.140625\n",
            "Batch: 44, Loss: 3.15847110748291, Accuracy: 0.1787109375\n",
            "Batch: 45, Loss: 3.1809654235839844, Accuracy: 0.16796875\n",
            "Batch: 46, Loss: 3.169527053833008, Accuracy: 0.1611328125\n",
            "Batch: 47, Loss: 3.208059787750244, Accuracy: 0.1494140625\n",
            "Batch: 48, Loss: 3.203305721282959, Accuracy: 0.1552734375\n",
            "Batch: 49, Loss: 3.2433691024780273, Accuracy: 0.150390625\n",
            "Batch: 50, Loss: 3.1417789459228516, Accuracy: 0.17578125\n",
            "Batch: 51, Loss: 3.143430709838867, Accuracy: 0.173828125\n",
            "Batch: 52, Loss: 3.146989583969116, Accuracy: 0.1943359375\n",
            "Batch: 53, Loss: 3.176513671875, Accuracy: 0.1865234375\n",
            "Batch: 54, Loss: 3.2473673820495605, Accuracy: 0.1552734375\n",
            "Batch: 55, Loss: 3.2616381645202637, Accuracy: 0.1533203125\n",
            "Batch: 56, Loss: 3.25126051902771, Accuracy: 0.1650390625\n",
            "Batch: 57, Loss: 3.184915065765381, Accuracy: 0.1689453125\n",
            "Batch: 58, Loss: 3.180166006088257, Accuracy: 0.1650390625\n",
            "Batch: 59, Loss: 3.323333501815796, Accuracy: 0.140625\n",
            "Batch: 60, Loss: 3.174666404724121, Accuracy: 0.1572265625\n",
            "Batch: 61, Loss: 3.1577847003936768, Accuracy: 0.16015625\n",
            "Batch: 62, Loss: 3.062990188598633, Accuracy: 0.171875\n",
            "Batch: 63, Loss: 3.174304962158203, Accuracy: 0.1708984375\n",
            "Batch: 64, Loss: 3.20926833152771, Accuracy: 0.16796875\n",
            "Batch: 65, Loss: 3.169387102127075, Accuracy: 0.169921875\n",
            "Batch: 66, Loss: 3.2196860313415527, Accuracy: 0.1728515625\n",
            "Batch: 67, Loss: 3.1602869033813477, Accuracy: 0.1591796875\n",
            "Batch: 68, Loss: 3.0630075931549072, Accuracy: 0.1787109375\n",
            "Batch: 69, Loss: 3.1391496658325195, Accuracy: 0.1611328125\n",
            "Batch: 70, Loss: 3.1324234008789062, Accuracy: 0.1787109375\n",
            "Batch: 71, Loss: 3.132533550262451, Accuracy: 0.1708984375\n",
            "Batch: 72, Loss: 3.2260615825653076, Accuracy: 0.158203125\n",
            "Batch: 73, Loss: 3.1319713592529297, Accuracy: 0.162109375\n",
            "Batch: 74, Loss: 3.109541893005371, Accuracy: 0.150390625\n",
            "Batch: 75, Loss: 3.042494535446167, Accuracy: 0.185546875\n",
            "Batch: 76, Loss: 3.052091121673584, Accuracy: 0.1767578125\n",
            "Batch: 77, Loss: 3.149984836578369, Accuracy: 0.1552734375\n",
            "Batch: 78, Loss: 3.132462978363037, Accuracy: 0.150390625\n",
            "Batch: 79, Loss: 3.095942974090576, Accuracy: 0.1474609375\n",
            "Batch: 80, Loss: 3.1073122024536133, Accuracy: 0.1650390625\n",
            "Batch: 81, Loss: 3.0567851066589355, Accuracy: 0.18359375\n",
            "Batch: 82, Loss: 3.1563668251037598, Accuracy: 0.1787109375\n",
            "Batch: 83, Loss: 3.11262583732605, Accuracy: 0.1630859375\n",
            "Batch: 84, Loss: 3.0003726482391357, Accuracy: 0.1845703125\n",
            "Batch: 85, Loss: 3.020054817199707, Accuracy: 0.17578125\n",
            "Batch: 86, Loss: 3.056553840637207, Accuracy: 0.17578125\n",
            "Batch: 87, Loss: 3.0515365600585938, Accuracy: 0.19140625\n",
            "Batch: 88, Loss: 2.998826026916504, Accuracy: 0.1875\n",
            "Batch: 89, Loss: 3.114537000656128, Accuracy: 0.1630859375\n",
            "Batch: 90, Loss: 3.0294723510742188, Accuracy: 0.1728515625\n",
            "Batch: 91, Loss: 3.03568172454834, Accuracy: 0.1826171875\n",
            "Batch: 92, Loss: 3.0439250469207764, Accuracy: 0.1611328125\n",
            "Batch: 93, Loss: 2.9801454544067383, Accuracy: 0.189453125\n",
            "Batch: 94, Loss: 2.9421229362487793, Accuracy: 0.1875\n",
            "Batch: 95, Loss: 2.9567112922668457, Accuracy: 0.1923828125\n",
            "Batch: 96, Loss: 2.928645610809326, Accuracy: 0.1845703125\n",
            "Batch: 97, Loss: 2.9680910110473633, Accuracy: 0.2080078125\n",
            "Batch: 98, Loss: 3.0286428928375244, Accuracy: 0.1875\n",
            "Batch: 99, Loss: 2.9512228965759277, Accuracy: 0.1865234375\n",
            "Batch: 100, Loss: 2.950413227081299, Accuracy: 0.2080078125\n",
            "Batch: 101, Loss: 2.8929715156555176, Accuracy: 0.1982421875\n",
            "Batch: 102, Loss: 2.9241933822631836, Accuracy: 0.1943359375\n",
            "Batch: 103, Loss: 3.018606662750244, Accuracy: 0.1669921875\n",
            "Batch: 104, Loss: 2.889601469039917, Accuracy: 0.19140625\n",
            "Batch: 105, Loss: 2.9664230346679688, Accuracy: 0.1904296875\n",
            "Batch: 106, Loss: 2.9272682666778564, Accuracy: 0.166015625\n",
            "Batch: 107, Loss: 2.9585492610931396, Accuracy: 0.197265625\n",
            "Batch: 108, Loss: 2.8250677585601807, Accuracy: 0.2001953125\n",
            "Batch: 109, Loss: 2.8578333854675293, Accuracy: 0.2177734375\n",
            "Batch: 110, Loss: 2.8560426235198975, Accuracy: 0.197265625\n",
            "Batch: 111, Loss: 2.8552684783935547, Accuracy: 0.2080078125\n",
            "Batch: 112, Loss: 2.8863625526428223, Accuracy: 0.2158203125\n",
            "Batch: 113, Loss: 2.8898696899414062, Accuracy: 0.2099609375\n",
            "Batch: 114, Loss: 2.8655219078063965, Accuracy: 0.1962890625\n",
            "Batch: 115, Loss: 2.981091022491455, Accuracy: 0.169921875\n",
            "Batch: 116, Loss: 2.9091899394989014, Accuracy: 0.1904296875\n",
            "Batch: 117, Loss: 2.9016833305358887, Accuracy: 0.18359375\n",
            "Batch: 118, Loss: 2.8123934268951416, Accuracy: 0.2080078125\n",
            "Batch: 119, Loss: 2.8301050662994385, Accuracy: 0.2060546875\n",
            "Batch: 120, Loss: 2.81880784034729, Accuracy: 0.2177734375\n",
            "Batch: 121, Loss: 2.687394380569458, Accuracy: 0.244140625\n",
            "Batch: 122, Loss: 2.7184834480285645, Accuracy: 0.2451171875\n",
            "Batch: 123, Loss: 2.8045434951782227, Accuracy: 0.21875\n",
            "Batch: 124, Loss: 2.8658246994018555, Accuracy: 0.2294921875\n",
            "Batch: 125, Loss: 2.6930184364318848, Accuracy: 0.267578125\n",
            "Batch: 126, Loss: 2.5787222385406494, Accuracy: 0.2587890625\n",
            "Batch: 127, Loss: 2.6777184009552, Accuracy: 0.2548828125\n",
            "Batch: 128, Loss: 2.6825339794158936, Accuracy: 0.2568359375\n",
            "Batch: 129, Loss: 2.7163712978363037, Accuracy: 0.24609375\n",
            "Batch: 130, Loss: 2.9128475189208984, Accuracy: 0.240234375\n",
            "Batch: 131, Loss: 2.7521791458129883, Accuracy: 0.2412109375\n",
            "Batch: 132, Loss: 2.6512765884399414, Accuracy: 0.2685546875\n",
            "Batch: 133, Loss: 2.66894268989563, Accuracy: 0.2578125\n",
            "Batch: 134, Loss: 2.7678747177124023, Accuracy: 0.2333984375\n",
            "Batch: 135, Loss: 2.719360113143921, Accuracy: 0.232421875\n",
            "Batch: 136, Loss: 2.6061267852783203, Accuracy: 0.2529296875\n",
            "Batch: 137, Loss: 2.6265432834625244, Accuracy: 0.267578125\n",
            "Batch: 138, Loss: 2.5586626529693604, Accuracy: 0.2919921875\n",
            "Batch: 139, Loss: 2.6163461208343506, Accuracy: 0.2685546875\n",
            "Batch: 140, Loss: 2.831205129623413, Accuracy: 0.248046875\n",
            "Batch: 141, Loss: 2.7159340381622314, Accuracy: 0.2685546875\n",
            "Batch: 142, Loss: 2.641680955886841, Accuracy: 0.2861328125\n",
            "Batch: 143, Loss: 2.6047725677490234, Accuracy: 0.2861328125\n",
            "Batch: 144, Loss: 2.6665616035461426, Accuracy: 0.2626953125\n",
            "Batch: 145, Loss: 2.6644222736358643, Accuracy: 0.244140625\n",
            "Batch: 146, Loss: 2.6779117584228516, Accuracy: 0.271484375\n",
            "Batch: 147, Loss: 2.593050241470337, Accuracy: 0.3095703125\n",
            "Batch: 148, Loss: 2.5493061542510986, Accuracy: 0.3212890625\n",
            "Batch: 149, Loss: 2.588682174682617, Accuracy: 0.33203125\n",
            "Batch: 150, Loss: 2.4923315048217773, Accuracy: 0.3251953125\n",
            "Batch: 151, Loss: 2.4992151260375977, Accuracy: 0.3056640625\n",
            "Batch: 152, Loss: 2.4554686546325684, Accuracy: 0.3330078125\n",
            "Batch: 153, Loss: 2.394747495651245, Accuracy: 0.3408203125\n",
            "Batch: 154, Loss: 2.439063549041748, Accuracy: 0.3232421875\n",
            "Batch: 155, Loss: 2.601440668106079, Accuracy: 0.3193359375\n",
            "Batch: 156, Loss: 2.577143907546997, Accuracy: 0.3212890625\n",
            "Batch: 157, Loss: 2.5575625896453857, Accuracy: 0.3466796875\n",
            "Batch: 158, Loss: 2.642117738723755, Accuracy: 0.322265625\n",
            "Batch: 159, Loss: 2.61271333694458, Accuracy: 0.2919921875\n",
            "Batch: 160, Loss: 2.590639352798462, Accuracy: 0.3076171875\n",
            "Batch: 161, Loss: 2.4359960556030273, Accuracy: 0.349609375\n",
            "Batch: 162, Loss: 2.552314519882202, Accuracy: 0.3125\n",
            "Batch: 163, Loss: 2.3750734329223633, Accuracy: 0.3564453125\n",
            "Batch: 164, Loss: 2.346872329711914, Accuracy: 0.3515625\n",
            "Batch: 165, Loss: 2.474961519241333, Accuracy: 0.3134765625\n",
            "Batch: 166, Loss: 2.575892686843872, Accuracy: 0.3076171875\n",
            "Batch: 167, Loss: 2.446441173553467, Accuracy: 0.32421875\n",
            "Batch: 168, Loss: 2.3627567291259766, Accuracy: 0.3291015625\n",
            "Batch: 169, Loss: 2.2301387786865234, Accuracy: 0.365234375\n",
            "Batch: 170, Loss: 2.390587329864502, Accuracy: 0.3466796875\n",
            "Batch: 171, Loss: 2.2854599952697754, Accuracy: 0.3603515625\n",
            "Batch: 172, Loss: 2.3728208541870117, Accuracy: 0.33984375\n",
            "Batch: 173, Loss: 2.495548725128174, Accuracy: 0.3291015625\n",
            "Batch: 174, Loss: 2.489546298980713, Accuracy: 0.2958984375\n",
            "Batch: 175, Loss: 2.6118531227111816, Accuracy: 0.3046875\n",
            "Batch: 176, Loss: 2.390395164489746, Accuracy: 0.3212890625\n",
            "Batch: 177, Loss: 2.3588764667510986, Accuracy: 0.37109375\n",
            "Batch: 178, Loss: 2.2638537883758545, Accuracy: 0.3916015625\n",
            "Batch: 179, Loss: 2.304001808166504, Accuracy: 0.3408203125\n",
            "Batch: 180, Loss: 2.4315361976623535, Accuracy: 0.3203125\n",
            "Batch: 181, Loss: 2.373643159866333, Accuracy: 0.37109375\n",
            "Batch: 182, Loss: 2.3616950511932373, Accuracy: 0.35546875\n",
            "Batch: 183, Loss: 2.318967342376709, Accuracy: 0.3662109375\n",
            "Batch: 184, Loss: 2.2976503372192383, Accuracy: 0.3564453125\n",
            "Batch: 185, Loss: 2.375814199447632, Accuracy: 0.33984375\n",
            "Batch: 186, Loss: 2.2420425415039062, Accuracy: 0.3603515625\n",
            "Batch: 187, Loss: 2.4070844650268555, Accuracy: 0.3388671875\n",
            "Batch: 188, Loss: 2.2493481636047363, Accuracy: 0.3740234375\n",
            "Epoch 2/5\n",
            "192782\n",
            "Batch: 1, Loss: 2.701829433441162, Accuracy: 0.2998046875\n",
            "Batch: 2, Loss: 2.3048853874206543, Accuracy: 0.3701171875\n",
            "Batch: 3, Loss: 2.3827390670776367, Accuracy: 0.3525390625\n",
            "Batch: 4, Loss: 2.3760945796966553, Accuracy: 0.357421875\n",
            "Batch: 5, Loss: 2.373429775238037, Accuracy: 0.3486328125\n",
            "Batch: 6, Loss: 2.511199951171875, Accuracy: 0.3349609375\n",
            "Batch: 7, Loss: 2.366013526916504, Accuracy: 0.3349609375\n",
            "Batch: 8, Loss: 2.3947386741638184, Accuracy: 0.337890625\n",
            "Batch: 9, Loss: 2.4350433349609375, Accuracy: 0.31640625\n",
            "Batch: 10, Loss: 2.4249114990234375, Accuracy: 0.32421875\n",
            "Batch: 11, Loss: 2.396350860595703, Accuracy: 0.3447265625\n",
            "Batch: 12, Loss: 2.3977484703063965, Accuracy: 0.32421875\n",
            "Batch: 13, Loss: 2.2183051109313965, Accuracy: 0.357421875\n",
            "Batch: 14, Loss: 2.2911462783813477, Accuracy: 0.3740234375\n",
            "Batch: 15, Loss: 2.286029577255249, Accuracy: 0.384765625\n",
            "Batch: 16, Loss: 2.2719357013702393, Accuracy: 0.3740234375\n",
            "Batch: 17, Loss: 2.2145299911499023, Accuracy: 0.3681640625\n",
            "Batch: 18, Loss: 2.255362033843994, Accuracy: 0.3603515625\n",
            "Batch: 19, Loss: 2.316502809524536, Accuracy: 0.3759765625\n",
            "Batch: 20, Loss: 2.217816114425659, Accuracy: 0.3935546875\n",
            "Batch: 21, Loss: 2.2212257385253906, Accuracy: 0.3857421875\n",
            "Batch: 22, Loss: 2.256985664367676, Accuracy: 0.380859375\n",
            "Batch: 23, Loss: 2.3485682010650635, Accuracy: 0.3642578125\n",
            "Batch: 24, Loss: 2.2060070037841797, Accuracy: 0.3896484375\n",
            "Batch: 25, Loss: 2.2710208892822266, Accuracy: 0.375\n",
            "Batch: 26, Loss: 2.1898441314697266, Accuracy: 0.4052734375\n",
            "Batch: 27, Loss: 2.288784980773926, Accuracy: 0.3798828125\n",
            "Batch: 28, Loss: 2.160980224609375, Accuracy: 0.3916015625\n",
            "Batch: 29, Loss: 2.153475046157837, Accuracy: 0.3837890625\n",
            "Batch: 30, Loss: 2.1950206756591797, Accuracy: 0.3994140625\n",
            "Batch: 31, Loss: 2.2174930572509766, Accuracy: 0.408203125\n",
            "Batch: 32, Loss: 2.170525550842285, Accuracy: 0.4111328125\n",
            "Batch: 33, Loss: 2.135723352432251, Accuracy: 0.408203125\n",
            "Batch: 34, Loss: 2.2772886753082275, Accuracy: 0.3759765625\n",
            "Batch: 35, Loss: 2.153233528137207, Accuracy: 0.41796875\n",
            "Batch: 36, Loss: 2.188322067260742, Accuracy: 0.3828125\n",
            "Batch: 37, Loss: 2.2264225482940674, Accuracy: 0.384765625\n",
            "Batch: 38, Loss: 2.3014585971832275, Accuracy: 0.380859375\n",
            "Batch: 39, Loss: 2.5004160404205322, Accuracy: 0.3173828125\n",
            "Batch: 40, Loss: 2.3830838203430176, Accuracy: 0.3544921875\n",
            "Batch: 41, Loss: 2.243292808532715, Accuracy: 0.390625\n",
            "Batch: 42, Loss: 2.231733798980713, Accuracy: 0.3642578125\n",
            "Batch: 43, Loss: 2.235337257385254, Accuracy: 0.390625\n",
            "Batch: 44, Loss: 2.021592378616333, Accuracy: 0.423828125\n",
            "Batch: 45, Loss: 2.02581524848938, Accuracy: 0.4130859375\n",
            "Batch: 46, Loss: 2.0579023361206055, Accuracy: 0.416015625\n",
            "Batch: 47, Loss: 2.089712142944336, Accuracy: 0.42578125\n",
            "Batch: 48, Loss: 2.000772476196289, Accuracy: 0.4169921875\n",
            "Batch: 49, Loss: 2.0695502758026123, Accuracy: 0.4013671875\n",
            "Batch: 50, Loss: 1.9052268266677856, Accuracy: 0.4345703125\n",
            "Batch: 51, Loss: 2.068970203399658, Accuracy: 0.4033203125\n",
            "Batch: 52, Loss: 1.987903356552124, Accuracy: 0.4169921875\n",
            "Batch: 53, Loss: 2.0763320922851562, Accuracy: 0.40234375\n",
            "Batch: 54, Loss: 2.2114498615264893, Accuracy: 0.384765625\n",
            "Batch: 55, Loss: 2.1794586181640625, Accuracy: 0.392578125\n",
            "Batch: 56, Loss: 2.2557616233825684, Accuracy: 0.3974609375\n",
            "Batch: 57, Loss: 2.165821075439453, Accuracy: 0.3857421875\n",
            "Batch: 58, Loss: 2.1921329498291016, Accuracy: 0.373046875\n",
            "Batch: 59, Loss: 2.266688823699951, Accuracy: 0.3564453125\n",
            "Batch: 60, Loss: 2.0317671298980713, Accuracy: 0.4326171875\n",
            "Batch: 61, Loss: 2.0607123374938965, Accuracy: 0.4150390625\n",
            "Batch: 62, Loss: 1.974647879600525, Accuracy: 0.416015625\n",
            "Batch: 63, Loss: 2.1322712898254395, Accuracy: 0.3857421875\n",
            "Batch: 64, Loss: 2.0808372497558594, Accuracy: 0.3623046875\n",
            "Batch: 65, Loss: 1.9996048212051392, Accuracy: 0.41015625\n",
            "Batch: 66, Loss: 2.1443891525268555, Accuracy: 0.3984375\n",
            "Batch: 67, Loss: 2.0596022605895996, Accuracy: 0.4091796875\n",
            "Batch: 68, Loss: 1.9405685663223267, Accuracy: 0.4248046875\n",
            "Batch: 69, Loss: 1.9944040775299072, Accuracy: 0.39453125\n",
            "Batch: 70, Loss: 2.077698230743408, Accuracy: 0.3740234375\n",
            "Batch: 71, Loss: 2.004037857055664, Accuracy: 0.4140625\n",
            "Batch: 72, Loss: 2.190808057785034, Accuracy: 0.4267578125\n",
            "Batch: 73, Loss: 2.152700901031494, Accuracy: 0.42578125\n",
            "Batch: 74, Loss: 2.0394608974456787, Accuracy: 0.384765625\n",
            "Batch: 75, Loss: 2.024722099304199, Accuracy: 0.416015625\n",
            "Batch: 76, Loss: 2.0252723693847656, Accuracy: 0.40625\n",
            "Batch: 77, Loss: 2.112339973449707, Accuracy: 0.4013671875\n",
            "Batch: 78, Loss: 2.0697219371795654, Accuracy: 0.42578125\n",
            "Batch: 79, Loss: 1.99149489402771, Accuracy: 0.421875\n",
            "Batch: 80, Loss: 2.0066089630126953, Accuracy: 0.4248046875\n",
            "Batch: 81, Loss: 2.0479512214660645, Accuracy: 0.4287109375\n",
            "Batch: 82, Loss: 2.2585225105285645, Accuracy: 0.4072265625\n",
            "Batch: 83, Loss: 2.1302497386932373, Accuracy: 0.4306640625\n",
            "Batch: 84, Loss: 1.9138543605804443, Accuracy: 0.4287109375\n",
            "Batch: 85, Loss: 1.9288756847381592, Accuracy: 0.447265625\n",
            "Batch: 86, Loss: 1.9923903942108154, Accuracy: 0.43359375\n",
            "Batch: 87, Loss: 2.0018997192382812, Accuracy: 0.4384765625\n",
            "Batch: 88, Loss: 2.0385591983795166, Accuracy: 0.412109375\n",
            "Batch: 89, Loss: 2.0545289516448975, Accuracy: 0.4326171875\n",
            "Batch: 90, Loss: 1.9976774454116821, Accuracy: 0.4287109375\n",
            "Batch: 91, Loss: 2.0450758934020996, Accuracy: 0.4208984375\n",
            "Batch: 92, Loss: 2.1003711223602295, Accuracy: 0.3916015625\n",
            "Batch: 93, Loss: 2.001279354095459, Accuracy: 0.419921875\n",
            "Batch: 94, Loss: 1.74412202835083, Accuracy: 0.462890625\n",
            "Batch: 95, Loss: 1.8643836975097656, Accuracy: 0.4697265625\n",
            "Batch: 96, Loss: 1.8331809043884277, Accuracy: 0.48046875\n",
            "Batch: 97, Loss: 1.9185078144073486, Accuracy: 0.4697265625\n",
            "Batch: 98, Loss: 2.0100438594818115, Accuracy: 0.4453125\n",
            "Batch: 99, Loss: 1.9794970750808716, Accuracy: 0.4423828125\n",
            "Batch: 100, Loss: 1.9032726287841797, Accuracy: 0.4580078125\n",
            "Batch: 101, Loss: 1.877097249031067, Accuracy: 0.4716796875\n",
            "Batch: 102, Loss: 1.939924955368042, Accuracy: 0.443359375\n",
            "Batch: 103, Loss: 2.0503416061401367, Accuracy: 0.427734375\n",
            "Batch: 104, Loss: 1.8902859687805176, Accuracy: 0.455078125\n",
            "Batch: 105, Loss: 1.9208691120147705, Accuracy: 0.458984375\n",
            "Batch: 106, Loss: 1.898868441581726, Accuracy: 0.4384765625\n",
            "Batch: 107, Loss: 1.9543426036834717, Accuracy: 0.44140625\n",
            "Batch: 108, Loss: 1.8145480155944824, Accuracy: 0.4912109375\n",
            "Batch: 109, Loss: 2.0370349884033203, Accuracy: 0.4404296875\n",
            "Batch: 110, Loss: 1.8252983093261719, Accuracy: 0.486328125\n",
            "Batch: 111, Loss: 1.8297263383865356, Accuracy: 0.474609375\n",
            "Batch: 112, Loss: 1.9845130443572998, Accuracy: 0.453125\n",
            "Batch: 113, Loss: 1.95587956905365, Accuracy: 0.4228515625\n",
            "Batch: 114, Loss: 1.9266043901443481, Accuracy: 0.4521484375\n",
            "Batch: 115, Loss: 1.8505884408950806, Accuracy: 0.484375\n",
            "Batch: 116, Loss: 1.8889901638031006, Accuracy: 0.4716796875\n",
            "Batch: 117, Loss: 1.9277513027191162, Accuracy: 0.474609375\n",
            "Batch: 118, Loss: 1.8539928197860718, Accuracy: 0.4736328125\n",
            "Batch: 119, Loss: 1.8147095441818237, Accuracy: 0.482421875\n",
            "Batch: 120, Loss: 1.828253984451294, Accuracy: 0.490234375\n",
            "Batch: 121, Loss: 1.7587034702301025, Accuracy: 0.482421875\n",
            "Batch: 122, Loss: 1.6789237260818481, Accuracy: 0.5185546875\n",
            "Batch: 123, Loss: 1.8295389413833618, Accuracy: 0.4833984375\n",
            "Batch: 124, Loss: 1.9043976068496704, Accuracy: 0.4775390625\n",
            "Batch: 125, Loss: 1.8056232929229736, Accuracy: 0.5\n",
            "Batch: 126, Loss: 1.765236258506775, Accuracy: 0.470703125\n",
            "Batch: 127, Loss: 1.848097324371338, Accuracy: 0.4677734375\n",
            "Batch: 128, Loss: 1.7792885303497314, Accuracy: 0.4697265625\n",
            "Batch: 129, Loss: 1.9302817583084106, Accuracy: 0.4658203125\n",
            "Batch: 130, Loss: 2.069456100463867, Accuracy: 0.4521484375\n",
            "Batch: 131, Loss: 1.9018805027008057, Accuracy: 0.4775390625\n",
            "Batch: 132, Loss: 1.8725796937942505, Accuracy: 0.4599609375\n",
            "Batch: 133, Loss: 1.831344723701477, Accuracy: 0.4970703125\n",
            "Batch: 134, Loss: 1.840246558189392, Accuracy: 0.4736328125\n",
            "Batch: 135, Loss: 1.730120301246643, Accuracy: 0.51171875\n",
            "Batch: 136, Loss: 1.6312694549560547, Accuracy: 0.5283203125\n",
            "Batch: 137, Loss: 1.875473976135254, Accuracy: 0.4609375\n",
            "Batch: 138, Loss: 1.6841306686401367, Accuracy: 0.517578125\n",
            "Batch: 139, Loss: 1.8005213737487793, Accuracy: 0.4814453125\n",
            "Batch: 140, Loss: 1.952693223953247, Accuracy: 0.474609375\n",
            "Batch: 141, Loss: 1.8427891731262207, Accuracy: 0.4697265625\n",
            "Batch: 142, Loss: 1.9346486330032349, Accuracy: 0.4423828125\n",
            "Batch: 143, Loss: 1.906076431274414, Accuracy: 0.44921875\n",
            "Batch: 144, Loss: 1.891721487045288, Accuracy: 0.4658203125\n",
            "Batch: 145, Loss: 1.8911771774291992, Accuracy: 0.4736328125\n",
            "Batch: 146, Loss: 1.8951127529144287, Accuracy: 0.4482421875\n",
            "Batch: 147, Loss: 1.9250969886779785, Accuracy: 0.4619140625\n",
            "Batch: 148, Loss: 1.9691965579986572, Accuracy: 0.4365234375\n",
            "Batch: 149, Loss: 1.8892624378204346, Accuracy: 0.4892578125\n",
            "Batch: 150, Loss: 1.7445330619812012, Accuracy: 0.5068359375\n",
            "Batch: 151, Loss: 1.7522172927856445, Accuracy: 0.505859375\n",
            "Batch: 152, Loss: 1.832302212715149, Accuracy: 0.4697265625\n",
            "Batch: 153, Loss: 1.757871150970459, Accuracy: 0.5029296875\n",
            "Batch: 154, Loss: 1.8297600746154785, Accuracy: 0.4853515625\n",
            "Batch: 155, Loss: 1.909015417098999, Accuracy: 0.4697265625\n",
            "Batch: 156, Loss: 1.9263757467269897, Accuracy: 0.490234375\n",
            "Batch: 157, Loss: 2.023070812225342, Accuracy: 0.4580078125\n",
            "Batch: 158, Loss: 2.0656912326812744, Accuracy: 0.4208984375\n",
            "Batch: 159, Loss: 1.9595980644226074, Accuracy: 0.4599609375\n",
            "Batch: 160, Loss: 1.966982126235962, Accuracy: 0.46875\n",
            "Batch: 161, Loss: 1.7681843042373657, Accuracy: 0.51171875\n",
            "Batch: 162, Loss: 1.8869417905807495, Accuracy: 0.48046875\n",
            "Batch: 163, Loss: 1.78144371509552, Accuracy: 0.4892578125\n",
            "Batch: 164, Loss: 1.8374665975570679, Accuracy: 0.455078125\n",
            "Batch: 165, Loss: 1.8935942649841309, Accuracy: 0.47265625\n",
            "Batch: 166, Loss: 1.9751787185668945, Accuracy: 0.46484375\n",
            "Batch: 167, Loss: 1.7575194835662842, Accuracy: 0.4951171875\n",
            "Batch: 168, Loss: 1.702645182609558, Accuracy: 0.5068359375\n",
            "Batch: 169, Loss: 1.6825668811798096, Accuracy: 0.501953125\n",
            "Batch: 170, Loss: 1.7816166877746582, Accuracy: 0.486328125\n",
            "Batch: 171, Loss: 1.8042083978652954, Accuracy: 0.466796875\n",
            "Batch: 172, Loss: 1.874773383140564, Accuracy: 0.4609375\n",
            "Batch: 173, Loss: 1.907019853591919, Accuracy: 0.4482421875\n",
            "Batch: 174, Loss: 1.898468255996704, Accuracy: 0.474609375\n",
            "Batch: 175, Loss: 1.9733408689498901, Accuracy: 0.4619140625\n",
            "Batch: 176, Loss: 1.8017241954803467, Accuracy: 0.4931640625\n",
            "Batch: 177, Loss: 1.8148233890533447, Accuracy: 0.484375\n",
            "Batch: 178, Loss: 1.8321378231048584, Accuracy: 0.4677734375\n",
            "Batch: 179, Loss: 1.8311500549316406, Accuracy: 0.458984375\n",
            "Batch: 180, Loss: 1.8020124435424805, Accuracy: 0.48828125\n",
            "Batch: 181, Loss: 1.7760354280471802, Accuracy: 0.501953125\n",
            "Batch: 182, Loss: 1.8765740394592285, Accuracy: 0.4619140625\n",
            "Batch: 183, Loss: 1.8563388586044312, Accuracy: 0.4609375\n",
            "Batch: 184, Loss: 1.8302149772644043, Accuracy: 0.4609375\n",
            "Batch: 185, Loss: 1.9385751485824585, Accuracy: 0.421875\n",
            "Batch: 186, Loss: 1.759360432624817, Accuracy: 0.4736328125\n",
            "Batch: 187, Loss: 1.840734601020813, Accuracy: 0.4609375\n",
            "Batch: 188, Loss: 1.8137472867965698, Accuracy: 0.466796875\n",
            "Epoch 3/5\n",
            "192782\n",
            "Batch: 1, Loss: 2.335777997970581, Accuracy: 0.390625\n",
            "Batch: 2, Loss: 1.9299530982971191, Accuracy: 0.431640625\n",
            "Batch: 3, Loss: 2.0028107166290283, Accuracy: 0.42578125\n",
            "Batch: 4, Loss: 1.8894176483154297, Accuracy: 0.4599609375\n",
            "Batch: 5, Loss: 1.8775137662887573, Accuracy: 0.4697265625\n",
            "Batch: 6, Loss: 2.017119884490967, Accuracy: 0.4150390625\n",
            "Batch: 7, Loss: 1.9054620265960693, Accuracy: 0.44140625\n",
            "Batch: 8, Loss: 1.8999626636505127, Accuracy: 0.4462890625\n",
            "Batch: 9, Loss: 1.9433667659759521, Accuracy: 0.4375\n",
            "Batch: 10, Loss: 1.9227311611175537, Accuracy: 0.4384765625\n",
            "Batch: 11, Loss: 1.852587103843689, Accuracy: 0.4736328125\n",
            "Batch: 12, Loss: 1.902327299118042, Accuracy: 0.4365234375\n",
            "Batch: 13, Loss: 1.7480992078781128, Accuracy: 0.46484375\n",
            "Batch: 14, Loss: 1.7901029586791992, Accuracy: 0.478515625\n",
            "Batch: 15, Loss: 1.7341632843017578, Accuracy: 0.5\n",
            "Batch: 16, Loss: 1.787832260131836, Accuracy: 0.49609375\n",
            "Batch: 17, Loss: 1.7788729667663574, Accuracy: 0.486328125\n",
            "Batch: 18, Loss: 1.750821590423584, Accuracy: 0.4970703125\n",
            "Batch: 19, Loss: 1.743227481842041, Accuracy: 0.4970703125\n",
            "Batch: 20, Loss: 1.684570550918579, Accuracy: 0.5126953125\n",
            "Batch: 21, Loss: 1.754231572151184, Accuracy: 0.48046875\n",
            "Batch: 22, Loss: 1.7704492807388306, Accuracy: 0.4912109375\n",
            "Batch: 23, Loss: 1.9698452949523926, Accuracy: 0.474609375\n",
            "Batch: 24, Loss: 1.8104456663131714, Accuracy: 0.50390625\n",
            "Batch: 25, Loss: 1.7833707332611084, Accuracy: 0.4873046875\n",
            "Batch: 26, Loss: 1.7583903074264526, Accuracy: 0.4853515625\n",
            "Batch: 27, Loss: 1.923050045967102, Accuracy: 0.4365234375\n",
            "Batch: 28, Loss: 1.7744252681732178, Accuracy: 0.4912109375\n",
            "Batch: 29, Loss: 1.6413460969924927, Accuracy: 0.533203125\n",
            "Batch: 30, Loss: 1.7144367694854736, Accuracy: 0.52734375\n",
            "Batch: 31, Loss: 1.7420392036437988, Accuracy: 0.5126953125\n",
            "Batch: 32, Loss: 1.7946795225143433, Accuracy: 0.509765625\n",
            "Batch: 33, Loss: 1.6535232067108154, Accuracy: 0.5341796875\n",
            "Batch: 34, Loss: 1.8319709300994873, Accuracy: 0.490234375\n",
            "Batch: 35, Loss: 1.7383207082748413, Accuracy: 0.505859375\n",
            "Batch: 36, Loss: 1.8369052410125732, Accuracy: 0.4609375\n",
            "Batch: 37, Loss: 1.6960656642913818, Accuracy: 0.529296875\n",
            "Batch: 38, Loss: 1.944369912147522, Accuracy: 0.4580078125\n",
            "Batch: 39, Loss: 2.032546043395996, Accuracy: 0.451171875\n",
            "Batch: 40, Loss: 1.9751641750335693, Accuracy: 0.462890625\n",
            "Batch: 41, Loss: 1.893968105316162, Accuracy: 0.482421875\n",
            "Batch: 42, Loss: 1.8034296035766602, Accuracy: 0.470703125\n",
            "Batch: 43, Loss: 1.7367033958435059, Accuracy: 0.5\n",
            "Batch: 44, Loss: 1.6304926872253418, Accuracy: 0.517578125\n",
            "Batch: 45, Loss: 1.6024976968765259, Accuracy: 0.5087890625\n",
            "Batch: 46, Loss: 1.614173412322998, Accuracy: 0.5302734375\n",
            "Batch: 47, Loss: 1.7084343433380127, Accuracy: 0.5185546875\n",
            "Batch: 48, Loss: 1.5317490100860596, Accuracy: 0.5439453125\n",
            "Batch: 49, Loss: 1.6113262176513672, Accuracy: 0.5419921875\n",
            "Batch: 50, Loss: 1.476729393005371, Accuracy: 0.5556640625\n",
            "Batch: 51, Loss: 1.6853975057601929, Accuracy: 0.509765625\n",
            "Batch: 52, Loss: 1.608899712562561, Accuracy: 0.521484375\n",
            "Batch: 53, Loss: 1.7139620780944824, Accuracy: 0.5078125\n",
            "Batch: 54, Loss: 1.7351126670837402, Accuracy: 0.51171875\n",
            "Batch: 55, Loss: 1.6893986463546753, Accuracy: 0.529296875\n",
            "Batch: 56, Loss: 1.8696982860565186, Accuracy: 0.5\n",
            "Batch: 57, Loss: 1.774753212928772, Accuracy: 0.509765625\n",
            "Batch: 58, Loss: 1.8749926090240479, Accuracy: 0.453125\n",
            "Batch: 59, Loss: 1.7934885025024414, Accuracy: 0.5087890625\n",
            "Batch: 60, Loss: 1.5490602254867554, Accuracy: 0.55859375\n",
            "Batch: 61, Loss: 1.6567723751068115, Accuracy: 0.51171875\n",
            "Batch: 62, Loss: 1.6520544290542603, Accuracy: 0.494140625\n",
            "Batch: 63, Loss: 1.767397403717041, Accuracy: 0.4814453125\n",
            "Batch: 64, Loss: 1.5632232427597046, Accuracy: 0.537109375\n",
            "Batch: 65, Loss: 1.6144906282424927, Accuracy: 0.5107421875\n",
            "Batch: 66, Loss: 1.7996883392333984, Accuracy: 0.490234375\n",
            "Batch: 67, Loss: 1.7536931037902832, Accuracy: 0.4677734375\n",
            "Batch: 68, Loss: 1.6676076650619507, Accuracy: 0.4892578125\n",
            "Batch: 69, Loss: 1.5598523616790771, Accuracy: 0.5263671875\n",
            "Batch: 70, Loss: 1.6343955993652344, Accuracy: 0.5146484375\n",
            "Batch: 71, Loss: 1.5910818576812744, Accuracy: 0.53515625\n",
            "Batch: 72, Loss: 1.7791221141815186, Accuracy: 0.5078125\n",
            "Batch: 73, Loss: 1.8142642974853516, Accuracy: 0.494140625\n",
            "Batch: 74, Loss: 1.5789865255355835, Accuracy: 0.5419921875\n",
            "Batch: 75, Loss: 1.6524062156677246, Accuracy: 0.5078125\n",
            "Batch: 76, Loss: 1.7164117097854614, Accuracy: 0.4775390625\n",
            "Batch: 77, Loss: 1.7367479801177979, Accuracy: 0.490234375\n",
            "Batch: 78, Loss: 1.6410572528839111, Accuracy: 0.5263671875\n",
            "Batch: 79, Loss: 1.6337664127349854, Accuracy: 0.51171875\n",
            "Batch: 80, Loss: 1.5824261903762817, Accuracy: 0.5341796875\n",
            "Batch: 81, Loss: 1.7690339088439941, Accuracy: 0.5029296875\n",
            "Batch: 82, Loss: 1.9384117126464844, Accuracy: 0.484375\n",
            "Batch: 83, Loss: 1.7684937715530396, Accuracy: 0.5009765625\n",
            "Batch: 84, Loss: 1.6283737421035767, Accuracy: 0.52734375\n",
            "Batch: 85, Loss: 1.55893874168396, Accuracy: 0.5400390625\n",
            "Batch: 86, Loss: 1.6431410312652588, Accuracy: 0.5322265625\n",
            "Batch: 87, Loss: 1.6360456943511963, Accuracy: 0.509765625\n",
            "Batch: 88, Loss: 1.6613715887069702, Accuracy: 0.4951171875\n",
            "Batch: 89, Loss: 1.6726782321929932, Accuracy: 0.5146484375\n",
            "Batch: 90, Loss: 1.6228125095367432, Accuracy: 0.5166015625\n",
            "Batch: 91, Loss: 1.6775541305541992, Accuracy: 0.5009765625\n",
            "Batch: 92, Loss: 1.7490546703338623, Accuracy: 0.4833984375\n",
            "Batch: 93, Loss: 1.6062065362930298, Accuracy: 0.5341796875\n",
            "Batch: 94, Loss: 1.4279625415802002, Accuracy: 0.5654296875\n",
            "Batch: 95, Loss: 1.5479928255081177, Accuracy: 0.541015625\n",
            "Batch: 96, Loss: 1.4841026067733765, Accuracy: 0.580078125\n",
            "Batch: 97, Loss: 1.6605074405670166, Accuracy: 0.515625\n",
            "Batch: 98, Loss: 1.6451916694641113, Accuracy: 0.5341796875\n",
            "Batch: 99, Loss: 1.659346580505371, Accuracy: 0.5087890625\n",
            "Batch: 100, Loss: 1.5519258975982666, Accuracy: 0.55078125\n",
            "Batch: 101, Loss: 1.5775774717330933, Accuracy: 0.533203125\n",
            "Batch: 102, Loss: 1.6979321241378784, Accuracy: 0.501953125\n",
            "Batch: 103, Loss: 1.7461626529693604, Accuracy: 0.498046875\n",
            "Batch: 104, Loss: 1.5948083400726318, Accuracy: 0.5205078125\n",
            "Batch: 105, Loss: 1.6066901683807373, Accuracy: 0.5400390625\n",
            "Batch: 106, Loss: 1.6515685319900513, Accuracy: 0.498046875\n",
            "Batch: 107, Loss: 1.6244535446166992, Accuracy: 0.5107421875\n",
            "Batch: 108, Loss: 1.456437587738037, Accuracy: 0.576171875\n",
            "Batch: 109, Loss: 1.8385601043701172, Accuracy: 0.482421875\n",
            "Batch: 110, Loss: 1.4883484840393066, Accuracy: 0.5830078125\n",
            "Batch: 111, Loss: 1.562525987625122, Accuracy: 0.5390625\n",
            "Batch: 112, Loss: 1.745086908340454, Accuracy: 0.4814453125\n",
            "Batch: 113, Loss: 1.7251945734024048, Accuracy: 0.4697265625\n",
            "Batch: 114, Loss: 1.6628334522247314, Accuracy: 0.521484375\n",
            "Batch: 115, Loss: 1.5027000904083252, Accuracy: 0.583984375\n",
            "Batch: 116, Loss: 1.6022529602050781, Accuracy: 0.529296875\n",
            "Batch: 117, Loss: 1.647521734237671, Accuracy: 0.5048828125\n",
            "Batch: 118, Loss: 1.623738408088684, Accuracy: 0.5087890625\n",
            "Batch: 119, Loss: 1.5562080144882202, Accuracy: 0.5458984375\n",
            "Batch: 120, Loss: 1.5466092824935913, Accuracy: 0.5458984375\n",
            "Batch: 121, Loss: 1.5631784200668335, Accuracy: 0.5087890625\n",
            "Batch: 122, Loss: 1.4299548864364624, Accuracy: 0.564453125\n",
            "Batch: 123, Loss: 1.5092798471450806, Accuracy: 0.5634765625\n",
            "Batch: 124, Loss: 1.6100910902023315, Accuracy: 0.552734375\n",
            "Batch: 125, Loss: 1.5477057695388794, Accuracy: 0.5498046875\n",
            "Batch: 126, Loss: 1.588592290878296, Accuracy: 0.5009765625\n",
            "Batch: 127, Loss: 1.5784800052642822, Accuracy: 0.5087890625\n",
            "Batch: 128, Loss: 1.5798792839050293, Accuracy: 0.5126953125\n",
            "Batch: 129, Loss: 1.6365653276443481, Accuracy: 0.525390625\n",
            "Batch: 130, Loss: 1.7376735210418701, Accuracy: 0.5263671875\n",
            "Batch: 131, Loss: 1.6772959232330322, Accuracy: 0.5029296875\n",
            "Batch: 132, Loss: 1.6600342988967896, Accuracy: 0.4931640625\n",
            "Batch: 133, Loss: 1.5197558403015137, Accuracy: 0.548828125\n",
            "Batch: 134, Loss: 1.540744423866272, Accuracy: 0.5458984375\n",
            "Batch: 135, Loss: 1.4188522100448608, Accuracy: 0.583984375\n",
            "Batch: 136, Loss: 1.377755045890808, Accuracy: 0.5830078125\n",
            "Batch: 137, Loss: 1.6741825342178345, Accuracy: 0.4853515625\n",
            "Batch: 138, Loss: 1.4107820987701416, Accuracy: 0.5673828125\n",
            "Batch: 139, Loss: 1.4863355159759521, Accuracy: 0.560546875\n",
            "Batch: 140, Loss: 1.5938215255737305, Accuracy: 0.5576171875\n",
            "Batch: 141, Loss: 1.581540822982788, Accuracy: 0.5400390625\n",
            "Batch: 142, Loss: 1.6917966604232788, Accuracy: 0.5068359375\n",
            "Batch: 143, Loss: 1.6923905611038208, Accuracy: 0.4970703125\n",
            "Batch: 144, Loss: 1.6064832210540771, Accuracy: 0.5185546875\n",
            "Batch: 145, Loss: 1.5650982856750488, Accuracy: 0.5439453125\n",
            "Batch: 146, Loss: 1.5978354215621948, Accuracy: 0.537109375\n",
            "Batch: 147, Loss: 1.6402528285980225, Accuracy: 0.5185546875\n",
            "Batch: 148, Loss: 1.7576452493667603, Accuracy: 0.4814453125\n",
            "Batch: 149, Loss: 1.6049326658248901, Accuracy: 0.537109375\n",
            "Batch: 150, Loss: 1.4821865558624268, Accuracy: 0.556640625\n",
            "Batch: 151, Loss: 1.4709787368774414, Accuracy: 0.5546875\n",
            "Batch: 152, Loss: 1.5360307693481445, Accuracy: 0.52734375\n",
            "Batch: 153, Loss: 1.519806146621704, Accuracy: 0.5458984375\n",
            "Batch: 154, Loss: 1.6162867546081543, Accuracy: 0.517578125\n",
            "Batch: 155, Loss: 1.6636779308319092, Accuracy: 0.501953125\n",
            "Batch: 156, Loss: 1.6733098030090332, Accuracy: 0.5302734375\n",
            "Batch: 157, Loss: 1.7808195352554321, Accuracy: 0.4951171875\n",
            "Batch: 158, Loss: 1.7866146564483643, Accuracy: 0.490234375\n",
            "Batch: 159, Loss: 1.6473426818847656, Accuracy: 0.5205078125\n",
            "Batch: 160, Loss: 1.6837706565856934, Accuracy: 0.5185546875\n",
            "Batch: 161, Loss: 1.5032434463500977, Accuracy: 0.5615234375\n",
            "Batch: 162, Loss: 1.6367324590682983, Accuracy: 0.5205078125\n",
            "Batch: 163, Loss: 1.5458035469055176, Accuracy: 0.5478515625\n",
            "Batch: 164, Loss: 1.6068350076675415, Accuracy: 0.5126953125\n",
            "Batch: 165, Loss: 1.667546033859253, Accuracy: 0.5205078125\n",
            "Batch: 166, Loss: 1.6348882913589478, Accuracy: 0.54296875\n",
            "Batch: 167, Loss: 1.4893556833267212, Accuracy: 0.5537109375\n",
            "Batch: 168, Loss: 1.4777106046676636, Accuracy: 0.5537109375\n",
            "Batch: 169, Loss: 1.4029490947723389, Accuracy: 0.5849609375\n",
            "Batch: 170, Loss: 1.5823105573654175, Accuracy: 0.521484375\n",
            "Batch: 171, Loss: 1.548206090927124, Accuracy: 0.5205078125\n",
            "Batch: 172, Loss: 1.5970511436462402, Accuracy: 0.5234375\n",
            "Batch: 173, Loss: 1.6142746210098267, Accuracy: 0.521484375\n",
            "Batch: 174, Loss: 1.5261788368225098, Accuracy: 0.560546875\n",
            "Batch: 175, Loss: 1.6813430786132812, Accuracy: 0.5283203125\n",
            "Batch: 176, Loss: 1.5535005331039429, Accuracy: 0.5341796875\n",
            "Batch: 177, Loss: 1.5952225923538208, Accuracy: 0.5205078125\n",
            "Batch: 178, Loss: 1.6396105289459229, Accuracy: 0.505859375\n",
            "Batch: 179, Loss: 1.6261012554168701, Accuracy: 0.4931640625\n",
            "Batch: 180, Loss: 1.3622671365737915, Accuracy: 0.6083984375\n",
            "Batch: 181, Loss: 1.5178428888320923, Accuracy: 0.55859375\n",
            "Batch: 182, Loss: 1.633217453956604, Accuracy: 0.5068359375\n",
            "Batch: 183, Loss: 1.606412410736084, Accuracy: 0.5341796875\n",
            "Batch: 184, Loss: 1.6175464391708374, Accuracy: 0.4951171875\n",
            "Batch: 185, Loss: 1.6373716592788696, Accuracy: 0.490234375\n",
            "Batch: 186, Loss: 1.4369317293167114, Accuracy: 0.583984375\n",
            "Batch: 187, Loss: 1.5252373218536377, Accuracy: 0.5439453125\n",
            "Batch: 188, Loss: 1.5676188468933105, Accuracy: 0.51171875\n",
            "Epoch 4/5\n",
            "192782\n",
            "Batch: 1, Loss: 1.9878920316696167, Accuracy: 0.4609375\n",
            "Batch: 2, Loss: 1.710732102394104, Accuracy: 0.470703125\n",
            "Batch: 3, Loss: 1.7106561660766602, Accuracy: 0.4814453125\n",
            "Batch: 4, Loss: 1.5331239700317383, Accuracy: 0.55078125\n",
            "Batch: 5, Loss: 1.5993098020553589, Accuracy: 0.5322265625\n",
            "Batch: 6, Loss: 1.733939290046692, Accuracy: 0.4794921875\n",
            "Batch: 7, Loss: 1.6541297435760498, Accuracy: 0.5\n",
            "Batch: 8, Loss: 1.5775812864303589, Accuracy: 0.5419921875\n",
            "Batch: 9, Loss: 1.6781299114227295, Accuracy: 0.50390625\n",
            "Batch: 10, Loss: 1.7003474235534668, Accuracy: 0.4951171875\n",
            "Batch: 11, Loss: 1.6058413982391357, Accuracy: 0.5009765625\n",
            "Batch: 12, Loss: 1.6473174095153809, Accuracy: 0.48046875\n",
            "Batch: 13, Loss: 1.4976471662521362, Accuracy: 0.5380859375\n",
            "Batch: 14, Loss: 1.5411888360977173, Accuracy: 0.5498046875\n",
            "Batch: 15, Loss: 1.4541187286376953, Accuracy: 0.57421875\n",
            "Batch: 16, Loss: 1.5723586082458496, Accuracy: 0.5322265625\n",
            "Batch: 17, Loss: 1.5831481218338013, Accuracy: 0.5146484375\n",
            "Batch: 18, Loss: 1.4919253587722778, Accuracy: 0.55859375\n",
            "Batch: 19, Loss: 1.4806896448135376, Accuracy: 0.5673828125\n",
            "Batch: 20, Loss: 1.3896509408950806, Accuracy: 0.59375\n",
            "Batch: 21, Loss: 1.5245628356933594, Accuracy: 0.5419921875\n",
            "Batch: 22, Loss: 1.5888643264770508, Accuracy: 0.5146484375\n",
            "Batch: 23, Loss: 1.72459077835083, Accuracy: 0.4951171875\n",
            "Batch: 24, Loss: 1.5730500221252441, Accuracy: 0.53515625\n",
            "Batch: 25, Loss: 1.5621205568313599, Accuracy: 0.53515625\n",
            "Batch: 26, Loss: 1.5688440799713135, Accuracy: 0.513671875\n",
            "Batch: 27, Loss: 1.7278088331222534, Accuracy: 0.4775390625\n",
            "Batch: 28, Loss: 1.584769368171692, Accuracy: 0.5185546875\n",
            "Batch: 29, Loss: 1.3945895433425903, Accuracy: 0.572265625\n",
            "Batch: 30, Loss: 1.4650317430496216, Accuracy: 0.580078125\n",
            "Batch: 31, Loss: 1.5250300168991089, Accuracy: 0.5380859375\n",
            "Batch: 32, Loss: 1.5764131546020508, Accuracy: 0.5322265625\n",
            "Batch: 33, Loss: 1.3926187753677368, Accuracy: 0.5693359375\n",
            "Batch: 34, Loss: 1.5911695957183838, Accuracy: 0.5224609375\n",
            "Batch: 35, Loss: 1.4979327917099, Accuracy: 0.533203125\n",
            "Batch: 36, Loss: 1.5762077569961548, Accuracy: 0.5224609375\n",
            "Batch: 37, Loss: 1.4675321578979492, Accuracy: 0.5732421875\n",
            "Batch: 38, Loss: 1.762202262878418, Accuracy: 0.4912109375\n",
            "Batch: 39, Loss: 1.782346487045288, Accuracy: 0.4970703125\n",
            "Batch: 40, Loss: 1.7904517650604248, Accuracy: 0.484375\n",
            "Batch: 41, Loss: 1.6858546733856201, Accuracy: 0.5009765625\n",
            "Batch: 42, Loss: 1.5508726835250854, Accuracy: 0.5283203125\n",
            "Batch: 43, Loss: 1.5174365043640137, Accuracy: 0.5400390625\n",
            "Batch: 44, Loss: 1.4449095726013184, Accuracy: 0.5439453125\n",
            "Batch: 45, Loss: 1.3794447183609009, Accuracy: 0.5556640625\n",
            "Batch: 46, Loss: 1.424619436264038, Accuracy: 0.5771484375\n",
            "Batch: 47, Loss: 1.496088981628418, Accuracy: 0.572265625\n",
            "Batch: 48, Loss: 1.349962830543518, Accuracy: 0.576171875\n",
            "Batch: 49, Loss: 1.426422357559204, Accuracy: 0.55859375\n",
            "Batch: 50, Loss: 1.267471432685852, Accuracy: 0.5966796875\n",
            "Batch: 51, Loss: 1.487365961074829, Accuracy: 0.544921875\n",
            "Batch: 52, Loss: 1.3956353664398193, Accuracy: 0.544921875\n",
            "Batch: 53, Loss: 1.4952738285064697, Accuracy: 0.5439453125\n",
            "Batch: 54, Loss: 1.4835081100463867, Accuracy: 0.560546875\n",
            "Batch: 55, Loss: 1.460654377937317, Accuracy: 0.5810546875\n",
            "Batch: 56, Loss: 1.636871099472046, Accuracy: 0.541015625\n",
            "Batch: 57, Loss: 1.5252985954284668, Accuracy: 0.537109375\n",
            "Batch: 58, Loss: 1.642212152481079, Accuracy: 0.490234375\n",
            "Batch: 59, Loss: 1.536384105682373, Accuracy: 0.552734375\n",
            "Batch: 60, Loss: 1.3538851737976074, Accuracy: 0.59765625\n",
            "Batch: 61, Loss: 1.4683687686920166, Accuracy: 0.5390625\n",
            "Batch: 62, Loss: 1.461504578590393, Accuracy: 0.5283203125\n",
            "Batch: 63, Loss: 1.542250156402588, Accuracy: 0.51953125\n",
            "Batch: 64, Loss: 1.3231959342956543, Accuracy: 0.5810546875\n",
            "Batch: 65, Loss: 1.4222575426101685, Accuracy: 0.5458984375\n",
            "Batch: 66, Loss: 1.5958597660064697, Accuracy: 0.5302734375\n",
            "Batch: 67, Loss: 1.5467126369476318, Accuracy: 0.5029296875\n",
            "Batch: 68, Loss: 1.4884452819824219, Accuracy: 0.521484375\n",
            "Batch: 69, Loss: 1.34641695022583, Accuracy: 0.58203125\n",
            "Batch: 70, Loss: 1.418814778327942, Accuracy: 0.5595703125\n",
            "Batch: 71, Loss: 1.3599505424499512, Accuracy: 0.5888671875\n",
            "Batch: 72, Loss: 1.558496356010437, Accuracy: 0.541015625\n",
            "Batch: 73, Loss: 1.618417501449585, Accuracy: 0.5146484375\n",
            "Batch: 74, Loss: 1.384636640548706, Accuracy: 0.576171875\n",
            "Batch: 75, Loss: 1.4545294046401978, Accuracy: 0.5556640625\n",
            "Batch: 76, Loss: 1.5221461057662964, Accuracy: 0.5234375\n",
            "Batch: 77, Loss: 1.5259901285171509, Accuracy: 0.529296875\n",
            "Batch: 78, Loss: 1.4181861877441406, Accuracy: 0.5634765625\n",
            "Batch: 79, Loss: 1.4421824216842651, Accuracy: 0.54296875\n",
            "Batch: 80, Loss: 1.3857721090316772, Accuracy: 0.5712890625\n",
            "Batch: 81, Loss: 1.5457433462142944, Accuracy: 0.5439453125\n",
            "Batch: 82, Loss: 1.7206993103027344, Accuracy: 0.515625\n",
            "Batch: 83, Loss: 1.5766112804412842, Accuracy: 0.513671875\n",
            "Batch: 84, Loss: 1.474141001701355, Accuracy: 0.5498046875\n",
            "Batch: 85, Loss: 1.4007091522216797, Accuracy: 0.556640625\n",
            "Batch: 86, Loss: 1.4379929304122925, Accuracy: 0.5498046875\n",
            "Batch: 87, Loss: 1.454840898513794, Accuracy: 0.552734375\n",
            "Batch: 88, Loss: 1.4988139867782593, Accuracy: 0.537109375\n",
            "Batch: 89, Loss: 1.4737001657485962, Accuracy: 0.5341796875\n",
            "Batch: 90, Loss: 1.4332268238067627, Accuracy: 0.5498046875\n",
            "Batch: 91, Loss: 1.4772415161132812, Accuracy: 0.5361328125\n",
            "Batch: 92, Loss: 1.5723776817321777, Accuracy: 0.5146484375\n",
            "Batch: 93, Loss: 1.417351245880127, Accuracy: 0.578125\n",
            "Batch: 94, Loss: 1.251711130142212, Accuracy: 0.599609375\n",
            "Batch: 95, Loss: 1.3673917055130005, Accuracy: 0.5830078125\n",
            "Batch: 96, Loss: 1.3343911170959473, Accuracy: 0.611328125\n",
            "Batch: 97, Loss: 1.4817619323730469, Accuracy: 0.55859375\n",
            "Batch: 98, Loss: 1.4478912353515625, Accuracy: 0.5625\n",
            "Batch: 99, Loss: 1.4961146116256714, Accuracy: 0.533203125\n",
            "Batch: 100, Loss: 1.4275012016296387, Accuracy: 0.5625\n",
            "Batch: 101, Loss: 1.425750732421875, Accuracy: 0.5615234375\n",
            "Batch: 102, Loss: 1.5394439697265625, Accuracy: 0.5234375\n",
            "Batch: 103, Loss: 1.5836706161499023, Accuracy: 0.5322265625\n",
            "Batch: 104, Loss: 1.4520434141159058, Accuracy: 0.5419921875\n",
            "Batch: 105, Loss: 1.4284539222717285, Accuracy: 0.5693359375\n",
            "Batch: 106, Loss: 1.4990733861923218, Accuracy: 0.525390625\n",
            "Batch: 107, Loss: 1.4618031978607178, Accuracy: 0.5478515625\n",
            "Batch: 108, Loss: 1.3203321695327759, Accuracy: 0.6025390625\n",
            "Batch: 109, Loss: 1.6933188438415527, Accuracy: 0.5068359375\n",
            "Batch: 110, Loss: 1.3257122039794922, Accuracy: 0.595703125\n",
            "Batch: 111, Loss: 1.4130275249481201, Accuracy: 0.5634765625\n",
            "Batch: 112, Loss: 1.5505000352859497, Accuracy: 0.5400390625\n",
            "Batch: 113, Loss: 1.5682830810546875, Accuracy: 0.494140625\n",
            "Batch: 114, Loss: 1.4973305463790894, Accuracy: 0.5517578125\n",
            "Batch: 115, Loss: 1.267748236656189, Accuracy: 0.615234375\n",
            "Batch: 116, Loss: 1.42103910446167, Accuracy: 0.55078125\n",
            "Batch: 117, Loss: 1.4545714855194092, Accuracy: 0.5517578125\n",
            "Batch: 118, Loss: 1.4383254051208496, Accuracy: 0.55078125\n",
            "Batch: 119, Loss: 1.3955999612808228, Accuracy: 0.5732421875\n",
            "Batch: 120, Loss: 1.3744854927062988, Accuracy: 0.580078125\n",
            "Batch: 121, Loss: 1.394580364227295, Accuracy: 0.5478515625\n",
            "Batch: 122, Loss: 1.2799463272094727, Accuracy: 0.58984375\n",
            "Batch: 123, Loss: 1.3230681419372559, Accuracy: 0.5869140625\n",
            "Batch: 124, Loss: 1.4126700162887573, Accuracy: 0.5849609375\n",
            "Batch: 125, Loss: 1.3506498336791992, Accuracy: 0.59375\n",
            "Batch: 126, Loss: 1.4476524591445923, Accuracy: 0.5205078125\n",
            "Batch: 127, Loss: 1.4728654623031616, Accuracy: 0.5322265625\n",
            "Batch: 128, Loss: 1.440020203590393, Accuracy: 0.5205078125\n",
            "Batch: 129, Loss: 1.5015645027160645, Accuracy: 0.5498046875\n",
            "Batch: 130, Loss: 1.5482629537582397, Accuracy: 0.552734375\n",
            "Batch: 131, Loss: 1.5113986730575562, Accuracy: 0.5439453125\n",
            "Batch: 132, Loss: 1.4913078546524048, Accuracy: 0.5146484375\n",
            "Batch: 133, Loss: 1.3614121675491333, Accuracy: 0.5859375\n",
            "Batch: 134, Loss: 1.3637899160385132, Accuracy: 0.580078125\n",
            "Batch: 135, Loss: 1.2689865827560425, Accuracy: 0.607421875\n",
            "Batch: 136, Loss: 1.2380295991897583, Accuracy: 0.6123046875\n",
            "Batch: 137, Loss: 1.5581413507461548, Accuracy: 0.5107421875\n",
            "Batch: 138, Loss: 1.2812142372131348, Accuracy: 0.5869140625\n",
            "Batch: 139, Loss: 1.3712694644927979, Accuracy: 0.5712890625\n",
            "Batch: 140, Loss: 1.437598705291748, Accuracy: 0.5888671875\n",
            "Batch: 141, Loss: 1.4082568883895874, Accuracy: 0.56640625\n",
            "Batch: 142, Loss: 1.5164158344268799, Accuracy: 0.5390625\n",
            "Batch: 143, Loss: 1.5365644693374634, Accuracy: 0.5126953125\n",
            "Batch: 144, Loss: 1.4702507257461548, Accuracy: 0.5341796875\n",
            "Batch: 145, Loss: 1.4255149364471436, Accuracy: 0.56640625\n",
            "Batch: 146, Loss: 1.4251644611358643, Accuracy: 0.5693359375\n",
            "Batch: 147, Loss: 1.4583088159561157, Accuracy: 0.544921875\n",
            "Batch: 148, Loss: 1.6167068481445312, Accuracy: 0.490234375\n",
            "Batch: 149, Loss: 1.4581623077392578, Accuracy: 0.5673828125\n",
            "Batch: 150, Loss: 1.3080365657806396, Accuracy: 0.5966796875\n",
            "Batch: 151, Loss: 1.3179411888122559, Accuracy: 0.607421875\n",
            "Batch: 152, Loss: 1.4039274454116821, Accuracy: 0.5556640625\n",
            "Batch: 153, Loss: 1.384610652923584, Accuracy: 0.5634765625\n",
            "Batch: 154, Loss: 1.4595531225204468, Accuracy: 0.55859375\n",
            "Batch: 155, Loss: 1.5105302333831787, Accuracy: 0.537109375\n",
            "Batch: 156, Loss: 1.4923282861709595, Accuracy: 0.568359375\n",
            "Batch: 157, Loss: 1.6114120483398438, Accuracy: 0.5224609375\n",
            "Batch: 158, Loss: 1.6225578784942627, Accuracy: 0.513671875\n",
            "Batch: 159, Loss: 1.4784399271011353, Accuracy: 0.568359375\n",
            "Batch: 160, Loss: 1.519988775253296, Accuracy: 0.5517578125\n",
            "Batch: 161, Loss: 1.3630359172821045, Accuracy: 0.576171875\n",
            "Batch: 162, Loss: 1.485761284828186, Accuracy: 0.5625\n",
            "Batch: 163, Loss: 1.4276559352874756, Accuracy: 0.55859375\n",
            "Batch: 164, Loss: 1.484594702720642, Accuracy: 0.53125\n",
            "Batch: 165, Loss: 1.541102409362793, Accuracy: 0.525390625\n",
            "Batch: 166, Loss: 1.4477049112319946, Accuracy: 0.5673828125\n",
            "Batch: 167, Loss: 1.3444056510925293, Accuracy: 0.578125\n",
            "Batch: 168, Loss: 1.333972454071045, Accuracy: 0.578125\n",
            "Batch: 169, Loss: 1.3059583902359009, Accuracy: 0.5810546875\n",
            "Batch: 170, Loss: 1.4556713104248047, Accuracy: 0.5400390625\n",
            "Batch: 171, Loss: 1.4443978071212769, Accuracy: 0.544921875\n",
            "Batch: 172, Loss: 1.4857627153396606, Accuracy: 0.546875\n",
            "Batch: 173, Loss: 1.4881689548492432, Accuracy: 0.5517578125\n",
            "Batch: 174, Loss: 1.380862832069397, Accuracy: 0.587890625\n",
            "Batch: 175, Loss: 1.55614173412323, Accuracy: 0.533203125\n",
            "Batch: 176, Loss: 1.4233207702636719, Accuracy: 0.568359375\n",
            "Batch: 177, Loss: 1.4455387592315674, Accuracy: 0.55078125\n",
            "Batch: 178, Loss: 1.509801983833313, Accuracy: 0.5283203125\n",
            "Batch: 179, Loss: 1.546128273010254, Accuracy: 0.5107421875\n",
            "Batch: 180, Loss: 1.2061176300048828, Accuracy: 0.6474609375\n",
            "Batch: 181, Loss: 1.3744841814041138, Accuracy: 0.5859375\n",
            "Batch: 182, Loss: 1.4752224683761597, Accuracy: 0.5439453125\n",
            "Batch: 183, Loss: 1.4758234024047852, Accuracy: 0.5361328125\n",
            "Batch: 184, Loss: 1.4753695726394653, Accuracy: 0.5361328125\n",
            "Batch: 185, Loss: 1.5306744575500488, Accuracy: 0.525390625\n",
            "Batch: 186, Loss: 1.3235739469528198, Accuracy: 0.607421875\n",
            "Batch: 187, Loss: 1.4072246551513672, Accuracy: 0.5693359375\n",
            "Batch: 188, Loss: 1.4600460529327393, Accuracy: 0.53125\n",
            "Epoch 5/5\n",
            "192782\n",
            "Batch: 1, Loss: 1.8015505075454712, Accuracy: 0.5078125\n",
            "Batch: 2, Loss: 1.599346399307251, Accuracy: 0.5009765625\n",
            "Batch: 3, Loss: 1.5795316696166992, Accuracy: 0.5\n",
            "Batch: 4, Loss: 1.409448266029358, Accuracy: 0.5888671875\n",
            "Batch: 5, Loss: 1.496558427810669, Accuracy: 0.54296875\n",
            "Batch: 6, Loss: 1.5997885465621948, Accuracy: 0.4990234375\n",
            "Batch: 7, Loss: 1.53449547290802, Accuracy: 0.5\n",
            "Batch: 8, Loss: 1.4206900596618652, Accuracy: 0.5732421875\n",
            "Batch: 9, Loss: 1.4867000579833984, Accuracy: 0.5224609375\n",
            "Batch: 10, Loss: 1.5146615505218506, Accuracy: 0.529296875\n",
            "Batch: 11, Loss: 1.4788920879364014, Accuracy: 0.521484375\n",
            "Batch: 12, Loss: 1.5363601446151733, Accuracy: 0.4921875\n",
            "Batch: 13, Loss: 1.400097131729126, Accuracy: 0.5498046875\n",
            "Batch: 14, Loss: 1.4042603969573975, Accuracy: 0.5732421875\n",
            "Batch: 15, Loss: 1.3176960945129395, Accuracy: 0.6044921875\n",
            "Batch: 16, Loss: 1.4610025882720947, Accuracy: 0.5732421875\n",
            "Batch: 17, Loss: 1.4871553182601929, Accuracy: 0.5419921875\n",
            "Batch: 18, Loss: 1.3769421577453613, Accuracy: 0.591796875\n",
            "Batch: 19, Loss: 1.3563354015350342, Accuracy: 0.6005859375\n",
            "Batch: 20, Loss: 1.271170973777771, Accuracy: 0.6201171875\n",
            "Batch: 21, Loss: 1.437550663948059, Accuracy: 0.5625\n",
            "Batch: 22, Loss: 1.4810713529586792, Accuracy: 0.5263671875\n",
            "Batch: 23, Loss: 1.5924879312515259, Accuracy: 0.5361328125\n",
            "Batch: 24, Loss: 1.4726673364639282, Accuracy: 0.5498046875\n",
            "Batch: 25, Loss: 1.443596601486206, Accuracy: 0.5439453125\n",
            "Batch: 26, Loss: 1.4381591081619263, Accuracy: 0.5458984375\n",
            "Batch: 27, Loss: 1.6150290966033936, Accuracy: 0.5029296875\n",
            "Batch: 28, Loss: 1.4712402820587158, Accuracy: 0.55078125\n",
            "Batch: 29, Loss: 1.2754920721054077, Accuracy: 0.58984375\n",
            "Batch: 30, Loss: 1.3469982147216797, Accuracy: 0.58203125\n",
            "Batch: 31, Loss: 1.3889936208724976, Accuracy: 0.5693359375\n",
            "Batch: 32, Loss: 1.444689393043518, Accuracy: 0.5537109375\n",
            "Batch: 33, Loss: 1.2936028242111206, Accuracy: 0.5927734375\n",
            "Batch: 34, Loss: 1.4287245273590088, Accuracy: 0.5556640625\n",
            "Batch: 35, Loss: 1.3764147758483887, Accuracy: 0.5439453125\n",
            "Batch: 36, Loss: 1.4607313871383667, Accuracy: 0.552734375\n",
            "Batch: 37, Loss: 1.3014363050460815, Accuracy: 0.595703125\n",
            "Batch: 38, Loss: 1.6213120222091675, Accuracy: 0.5224609375\n",
            "Batch: 39, Loss: 1.5898293256759644, Accuracy: 0.529296875\n",
            "Batch: 40, Loss: 1.6631993055343628, Accuracy: 0.51953125\n",
            "Batch: 41, Loss: 1.5743467807769775, Accuracy: 0.5126953125\n",
            "Batch: 42, Loss: 1.443873643875122, Accuracy: 0.541015625\n",
            "Batch: 43, Loss: 1.4233375787734985, Accuracy: 0.5556640625\n",
            "Batch: 44, Loss: 1.3404970169067383, Accuracy: 0.5732421875\n",
            "Batch: 45, Loss: 1.2847715616226196, Accuracy: 0.5859375\n",
            "Batch: 46, Loss: 1.3243036270141602, Accuracy: 0.5947265625\n",
            "Batch: 47, Loss: 1.3910163640975952, Accuracy: 0.5810546875\n",
            "Batch: 48, Loss: 1.228123426437378, Accuracy: 0.609375\n",
            "Batch: 49, Loss: 1.2903932332992554, Accuracy: 0.599609375\n",
            "Batch: 50, Loss: 1.1766648292541504, Accuracy: 0.6142578125\n",
            "Batch: 51, Loss: 1.4006367921829224, Accuracy: 0.5498046875\n",
            "Batch: 52, Loss: 1.2681986093521118, Accuracy: 0.5908203125\n",
            "Batch: 53, Loss: 1.403331995010376, Accuracy: 0.5625\n",
            "Batch: 54, Loss: 1.3738527297973633, Accuracy: 0.576171875\n",
            "Batch: 55, Loss: 1.3285324573516846, Accuracy: 0.599609375\n",
            "Batch: 56, Loss: 1.5184866189956665, Accuracy: 0.5712890625\n",
            "Batch: 57, Loss: 1.4087913036346436, Accuracy: 0.568359375\n",
            "Batch: 58, Loss: 1.5054476261138916, Accuracy: 0.5283203125\n",
            "Batch: 59, Loss: 1.3835147619247437, Accuracy: 0.5908203125\n",
            "Batch: 60, Loss: 1.2356576919555664, Accuracy: 0.611328125\n",
            "Batch: 61, Loss: 1.3830112218856812, Accuracy: 0.56640625\n",
            "Batch: 62, Loss: 1.3612494468688965, Accuracy: 0.5322265625\n",
            "Batch: 63, Loss: 1.4566587209701538, Accuracy: 0.53515625\n",
            "Batch: 64, Loss: 1.1975516080856323, Accuracy: 0.5986328125\n",
            "Batch: 65, Loss: 1.3143038749694824, Accuracy: 0.5712890625\n",
            "Batch: 66, Loss: 1.4865739345550537, Accuracy: 0.5615234375\n",
            "Batch: 67, Loss: 1.468772053718567, Accuracy: 0.50390625\n",
            "Batch: 68, Loss: 1.3736259937286377, Accuracy: 0.552734375\n",
            "Batch: 69, Loss: 1.2612769603729248, Accuracy: 0.5888671875\n",
            "Batch: 70, Loss: 1.3028998374938965, Accuracy: 0.5732421875\n",
            "Batch: 71, Loss: 1.2397606372833252, Accuracy: 0.6123046875\n",
            "Batch: 72, Loss: 1.4396038055419922, Accuracy: 0.5751953125\n",
            "Batch: 73, Loss: 1.5375096797943115, Accuracy: 0.525390625\n",
            "Batch: 74, Loss: 1.262673020362854, Accuracy: 0.595703125\n",
            "Batch: 75, Loss: 1.363984227180481, Accuracy: 0.5771484375\n",
            "Batch: 76, Loss: 1.4251551628112793, Accuracy: 0.5390625\n",
            "Batch: 77, Loss: 1.3881199359893799, Accuracy: 0.56640625\n",
            "Batch: 78, Loss: 1.2990245819091797, Accuracy: 0.6005859375\n",
            "Batch: 79, Loss: 1.3255417346954346, Accuracy: 0.57421875\n",
            "Batch: 80, Loss: 1.2788832187652588, Accuracy: 0.611328125\n",
            "Batch: 81, Loss: 1.466979742050171, Accuracy: 0.5458984375\n",
            "Batch: 82, Loss: 1.6168475151062012, Accuracy: 0.529296875\n",
            "Batch: 83, Loss: 1.4756640195846558, Accuracy: 0.533203125\n",
            "Batch: 84, Loss: 1.3934252262115479, Accuracy: 0.552734375\n",
            "Batch: 85, Loss: 1.2858225107192993, Accuracy: 0.591796875\n",
            "Batch: 86, Loss: 1.346565842628479, Accuracy: 0.5703125\n",
            "Batch: 87, Loss: 1.3498115539550781, Accuracy: 0.5771484375\n",
            "Batch: 88, Loss: 1.3903048038482666, Accuracy: 0.5439453125\n",
            "Batch: 89, Loss: 1.3718128204345703, Accuracy: 0.55859375\n",
            "Batch: 90, Loss: 1.3352816104888916, Accuracy: 0.5751953125\n",
            "Batch: 91, Loss: 1.3728597164154053, Accuracy: 0.56640625\n",
            "Batch: 92, Loss: 1.4550431966781616, Accuracy: 0.541015625\n",
            "Batch: 93, Loss: 1.2889750003814697, Accuracy: 0.59375\n",
            "Batch: 94, Loss: 1.1394459009170532, Accuracy: 0.61328125\n",
            "Batch: 95, Loss: 1.2671869993209839, Accuracy: 0.58984375\n",
            "Batch: 96, Loss: 1.2324819564819336, Accuracy: 0.62109375\n",
            "Batch: 97, Loss: 1.398071050643921, Accuracy: 0.5537109375\n",
            "Batch: 98, Loss: 1.3550671339035034, Accuracy: 0.58984375\n",
            "Batch: 99, Loss: 1.408519983291626, Accuracy: 0.556640625\n",
            "Batch: 100, Loss: 1.313938856124878, Accuracy: 0.5869140625\n",
            "Batch: 101, Loss: 1.3272089958190918, Accuracy: 0.5751953125\n",
            "Batch: 102, Loss: 1.4763832092285156, Accuracy: 0.5419921875\n",
            "Batch: 103, Loss: 1.4423866271972656, Accuracy: 0.568359375\n",
            "Batch: 104, Loss: 1.3495930433273315, Accuracy: 0.5439453125\n",
            "Batch: 105, Loss: 1.3190104961395264, Accuracy: 0.5927734375\n",
            "Batch: 106, Loss: 1.3971948623657227, Accuracy: 0.5390625\n",
            "Batch: 107, Loss: 1.3662265539169312, Accuracy: 0.5703125\n",
            "Batch: 108, Loss: 1.2207915782928467, Accuracy: 0.6240234375\n",
            "Batch: 109, Loss: 1.6226328611373901, Accuracy: 0.5185546875\n",
            "Batch: 110, Loss: 1.2455832958221436, Accuracy: 0.625\n",
            "Batch: 111, Loss: 1.3179700374603271, Accuracy: 0.5830078125\n",
            "Batch: 112, Loss: 1.49635648727417, Accuracy: 0.5458984375\n",
            "Batch: 113, Loss: 1.5077916383743286, Accuracy: 0.5244140625\n",
            "Batch: 114, Loss: 1.421203374862671, Accuracy: 0.5732421875\n",
            "Batch: 115, Loss: 1.1755746603012085, Accuracy: 0.65234375\n",
            "Batch: 116, Loss: 1.3252778053283691, Accuracy: 0.58203125\n",
            "Batch: 117, Loss: 1.4072391986846924, Accuracy: 0.56640625\n",
            "Batch: 118, Loss: 1.374335765838623, Accuracy: 0.5498046875\n",
            "Batch: 119, Loss: 1.2905519008636475, Accuracy: 0.59765625\n",
            "Batch: 120, Loss: 1.270646572113037, Accuracy: 0.595703125\n",
            "Batch: 121, Loss: 1.301509976387024, Accuracy: 0.564453125\n",
            "Batch: 122, Loss: 1.1789425611495972, Accuracy: 0.619140625\n",
            "Batch: 123, Loss: 1.2611110210418701, Accuracy: 0.6162109375\n",
            "Batch: 124, Loss: 1.2857353687286377, Accuracy: 0.6083984375\n",
            "Batch: 125, Loss: 1.2610487937927246, Accuracy: 0.6015625\n",
            "Batch: 126, Loss: 1.3528151512145996, Accuracy: 0.548828125\n",
            "Batch: 127, Loss: 1.3801250457763672, Accuracy: 0.5498046875\n",
            "Batch: 128, Loss: 1.384000539779663, Accuracy: 0.560546875\n",
            "Batch: 129, Loss: 1.393796682357788, Accuracy: 0.5732421875\n",
            "Batch: 130, Loss: 1.450425386428833, Accuracy: 0.5712890625\n",
            "Batch: 131, Loss: 1.4163309335708618, Accuracy: 0.546875\n",
            "Batch: 132, Loss: 1.3927223682403564, Accuracy: 0.54296875\n",
            "Batch: 133, Loss: 1.2639926671981812, Accuracy: 0.6064453125\n",
            "Batch: 134, Loss: 1.2663861513137817, Accuracy: 0.6044921875\n",
            "Batch: 135, Loss: 1.171083927154541, Accuracy: 0.640625\n",
            "Batch: 136, Loss: 1.1658631563186646, Accuracy: 0.638671875\n",
            "Batch: 137, Loss: 1.4609928131103516, Accuracy: 0.53515625\n",
            "Batch: 138, Loss: 1.1878724098205566, Accuracy: 0.6220703125\n",
            "Batch: 139, Loss: 1.288931965827942, Accuracy: 0.591796875\n",
            "Batch: 140, Loss: 1.3489407300949097, Accuracy: 0.603515625\n",
            "Batch: 141, Loss: 1.3080346584320068, Accuracy: 0.5966796875\n",
            "Batch: 142, Loss: 1.4240806102752686, Accuracy: 0.564453125\n",
            "Batch: 143, Loss: 1.4400477409362793, Accuracy: 0.5380859375\n",
            "Batch: 144, Loss: 1.3867864608764648, Accuracy: 0.55078125\n",
            "Batch: 145, Loss: 1.3398199081420898, Accuracy: 0.5703125\n",
            "Batch: 146, Loss: 1.3131279945373535, Accuracy: 0.583984375\n",
            "Batch: 147, Loss: 1.366825819015503, Accuracy: 0.556640625\n",
            "Batch: 148, Loss: 1.5604853630065918, Accuracy: 0.498046875\n",
            "Batch: 149, Loss: 1.4074567556381226, Accuracy: 0.564453125\n",
            "Batch: 150, Loss: 1.2218539714813232, Accuracy: 0.625\n",
            "Batch: 151, Loss: 1.2378166913986206, Accuracy: 0.611328125\n",
            "Batch: 152, Loss: 1.3008391857147217, Accuracy: 0.5830078125\n",
            "Batch: 153, Loss: 1.3147858381271362, Accuracy: 0.5859375\n",
            "Batch: 154, Loss: 1.3695951700210571, Accuracy: 0.576171875\n",
            "Batch: 155, Loss: 1.3805283308029175, Accuracy: 0.5546875\n",
            "Batch: 156, Loss: 1.4002233743667603, Accuracy: 0.568359375\n",
            "Batch: 157, Loss: 1.5381532907485962, Accuracy: 0.53515625\n",
            "Batch: 158, Loss: 1.512013554573059, Accuracy: 0.53515625\n",
            "Batch: 159, Loss: 1.3996392488479614, Accuracy: 0.56640625\n",
            "Batch: 160, Loss: 1.434632420539856, Accuracy: 0.5654296875\n",
            "Batch: 161, Loss: 1.2954132556915283, Accuracy: 0.6064453125\n",
            "Batch: 162, Loss: 1.4247398376464844, Accuracy: 0.57421875\n",
            "Batch: 163, Loss: 1.331810712814331, Accuracy: 0.58203125\n",
            "Batch: 164, Loss: 1.3779656887054443, Accuracy: 0.556640625\n",
            "Batch: 165, Loss: 1.4476754665374756, Accuracy: 0.5537109375\n",
            "Batch: 166, Loss: 1.326838493347168, Accuracy: 0.59765625\n",
            "Batch: 167, Loss: 1.2601574659347534, Accuracy: 0.59765625\n",
            "Batch: 168, Loss: 1.2492504119873047, Accuracy: 0.607421875\n",
            "Batch: 169, Loss: 1.2374485731124878, Accuracy: 0.59375\n",
            "Batch: 170, Loss: 1.3485723733901978, Accuracy: 0.5908203125\n",
            "Batch: 171, Loss: 1.388789176940918, Accuracy: 0.5595703125\n",
            "Batch: 172, Loss: 1.4206560850143433, Accuracy: 0.560546875\n",
            "Batch: 173, Loss: 1.4096488952636719, Accuracy: 0.57421875\n",
            "Batch: 174, Loss: 1.2526583671569824, Accuracy: 0.6240234375\n",
            "Batch: 175, Loss: 1.448825716972351, Accuracy: 0.5732421875\n",
            "Batch: 176, Loss: 1.322549819946289, Accuracy: 0.5966796875\n",
            "Batch: 177, Loss: 1.3194831609725952, Accuracy: 0.5966796875\n",
            "Batch: 178, Loss: 1.442048192024231, Accuracy: 0.5595703125\n",
            "Batch: 179, Loss: 1.4640021324157715, Accuracy: 0.53515625\n",
            "Batch: 180, Loss: 1.1168142557144165, Accuracy: 0.6650390625\n",
            "Batch: 181, Loss: 1.2602468729019165, Accuracy: 0.6103515625\n",
            "Batch: 182, Loss: 1.3537795543670654, Accuracy: 0.560546875\n",
            "Batch: 183, Loss: 1.3792378902435303, Accuracy: 0.568359375\n",
            "Batch: 184, Loss: 1.3558335304260254, Accuracy: 0.5478515625\n",
            "Batch: 185, Loss: 1.4030214548110962, Accuracy: 0.556640625\n",
            "Batch: 186, Loss: 1.230016827583313, Accuracy: 0.62109375\n",
            "Batch: 187, Loss: 1.2904727458953857, Accuracy: 0.5986328125\n",
            "Batch: 188, Loss: 1.3789234161376953, Accuracy: 0.55859375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "WhHM8isurpo0",
        "outputId": "8c0cb510-a4da-42ef-9d8d-ae64b9c2e516"
      },
      "source": [
        "log = pd.read_csv(os.path.join(data_directory, \"log.csv\"))\n",
        "log"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Epoch</th>\n",
              "      <th>Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2.223322</td>\n",
              "      <td>0.385742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1.767112</td>\n",
              "      <td>0.473633</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1.565153</td>\n",
              "      <td>0.517578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1.470443</td>\n",
              "      <td>0.541016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1.387324</td>\n",
              "      <td>0.565430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>196</td>\n",
              "      <td>0.212401</td>\n",
              "      <td>0.928711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>197</td>\n",
              "      <td>0.192337</td>\n",
              "      <td>0.943359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>198</td>\n",
              "      <td>0.199914</td>\n",
              "      <td>0.935547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>199</td>\n",
              "      <td>0.199582</td>\n",
              "      <td>0.940430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>200</td>\n",
              "      <td>0.203088</td>\n",
              "      <td>0.932617</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows  3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Epoch      Loss  Accuracy\n",
              "0        1  2.223322  0.385742\n",
              "1        2  1.767112  0.473633\n",
              "2        3  1.565153  0.517578\n",
              "3        4  1.470443  0.541016\n",
              "4        5  1.387324  0.565430\n",
              "..     ...       ...       ...\n",
              "195    196  0.212401  0.928711\n",
              "196    197  0.192337  0.943359\n",
              "197    198  0.199914  0.935547\n",
              "198    199  0.199582  0.940430\n",
              "199    200  0.203088  0.932617\n",
              "\n",
              "[200 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "s_YeRwxJqC60",
        "outputId": "132c714b-b338-4321-eefd-db2f0f6fe18c"
      },
      "source": [
        "length\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-29448fdbe8f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'length' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu3utD6JKjm0"
      },
      "source": [
        "def make_model(unique_chars):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(input_dim = unique_chars, output_dim = 512, batch_input_shape = (1, 1))) \n",
        "  \n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(LSTM(256, return_sequences = True, stateful = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(LSTM(256, stateful = True)) \n",
        "   \n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add((Dense(unique_chars)))\n",
        "    model.add(Activation(\"softmax\"))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ADYjyZPJYJR"
      },
      "source": [
        "def generate_sequence(epoch_num, initial_index, seq_length):\n",
        "    with open(os.path.join(data_directory, charIndex_json)) as f:\n",
        "        char_to_index = json.load(f)\n",
        "    index_to_char = {i:ch for ch, i in char_to_index.items()}\n",
        "    unique_chars = len(index_to_char)\n",
        "    \n",
        "    model = make_model(unique_chars)\n",
        "    model.load_weights(os.path.join(model_weights_directory , \"Weights_{}.h5\".format(epoch_num)))\n",
        "     \n",
        "    sequence_index = [initial_index]\n",
        "    \n",
        "    for _ in range(seq_length):\n",
        "        batch = np.zeros((1, 1))\n",
        "        batch[0, 0] = sequence_index[-1]\n",
        "        \n",
        "        predicted_probs = model.predict_on_batch(batch).ravel()\n",
        "        sample = np.random.choice(range(unique_chars), size = 1, p = predicted_probs)\n",
        "        \n",
        "        sequence_index.append(sample[0])\n",
        "    \n",
        "    seq = ''.join(index_to_char[c] for c in sequence_index)\n",
        "    \n",
        "    cnt = 0\n",
        "    for i in seq:\n",
        "        cnt += 1\n",
        "        if i == \"\\n\":\n",
        "            break\n",
        "    seq1 = seq[cnt:]\n",
        "   \n",
        "    \n",
        "    cnt = 0\n",
        "    for i in seq1:\n",
        "        cnt += 1\n",
        "        if i == \"\\n\" and seq1[cnt] == \"\\n\":\n",
        "            break\n",
        "    seq2 = seq1[:cnt]\n",
        "    \n",
        "    \n",
        "    return seq2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9R8ZQRPkKAE0",
        "outputId": "fba0caeb-9593-44e3-a420-d03399f25d33"
      },
      "source": [
        "ep = int(input(\"1. Which epoch number weight you want to load into the model(10, 20, 30, ..., 90). Small number will generate more errors in music: \"))\n",
        "ar = int(input(\"\\n2. Enter any number between 0 to 86 which will be given as initial charcter to model for generating sequence: \"))\n",
        "ln = int(input(\"\\n3. Enter the length of music sequence you want to generate. Typical number is between 300-600. Too small number will generate hardly generate any sequence: \"))\n",
        "\n",
        "music = generate_sequence(ep, ar, ln)\n",
        "\n",
        "print(\"\\nMUSIC SEQUENCE GENERATED: \\n\")\n",
        "\n",
        "print(music)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Which epoch number weight you want to load into the model(10, 20, 30, ..., 90). Small number will generate more errors in music: 200\n",
            "\n",
            "2. Enter any number between 0 to 86 which will be given as initial charcter to model for generating sequence: 69\n",
            "\n",
            "3. Enter the length of music sequence you want to generate. Typical number is between 300-600. Too small number will generate hardly generate any sequence: 1500\n",
            "\n",
            "MUSIC SEQUENCE GENERATED: \n",
            "\n",
            "\"A\"cBA E2E|\"D\"F2A def|\"Em\"g3 \"Bm\"f2e|\"Em\"dcd \"A7\"ecA|\\\n",
            "\"D\"d3 \"A7\"d2:|\n",
            "P:B\n",
            "c/2d/2|\"D\"f2a a3|\"A7\"fgf ecA|\"D\"faf \"G\"g2f|\"D/f+\"a3 \"G\"b2g|\"D\"f2f fed|\"Em\"g2e \"A7\"f2g|\n",
            "\"D\"a2f \"A7\"g2e|\"D\"a2f \"G\"b2g|\"D\"afd \"Bm\"d2f|\"Em\"e2B \"Bm\"B2d|\"A7\"cBc \"D\"d2:|\n",
            "P:B\n",
            "c/2d/2|\"C\"e2e ece|\"G\"d2g \"Am\"e2c|\"G\"dcB \"D7\"AGF|\"G\"G3 G2:|\n",
            "K:D\n",
            "P:B\n",
            "f/2g/2|\"D\"a2f d2e|\"D\"fgf \"A/c+\"e2e|\"Bm\"d2d \"B7/a\"B2c|\"Em\"ded \"A7\"cBA|\"D\"d3 d2:|\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDaGaUlQKUVa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}